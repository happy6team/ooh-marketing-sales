from typing import TypedDict, Optional
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import mysql.connector
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from docx import Document
import pandas as pd
import os
from transformers import AutoTokenizer, AutoModel
import torch
import json
from decimal import Decimal
import sys
import time
from sqlalchemy import text
import asyncio

from db import AsyncSessionLocal

# --- ğŸ” ìƒíƒœ ì •ì˜ ---
class ProposalState(TypedDict, total=False):
    brand_name: Optional[str]
    brand_info: Optional[str]
    client_needs: Optional[str]
    recent_issues: Optional[str]
    sales_status: Optional[str]
    recommended_media: Optional[str]
    previous_campaigns: Optional[str]
    proposal_text: Optional[str]
    proposal_file_path: Optional[str]
    media_info: list

AgentState = ProposalState

# --- .env ì—ì„œ OPENAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key)

# --- HuggingFace ê¸°ë°˜ ì„ë² ë”© í´ë˜ìŠ¤ ---
class BERTSentenceEmbedding:
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

    def embed_documents(self, texts):
        return [self._embed(text) for text in texts]

    def embed_query(self, text):
        return self._embed(text)

    def _embed(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        return cls_embedding.squeeze(0).cpu().numpy()

embedding_function = BERTSentenceEmbedding()

# --- âœ… ê¸°ì¡´ ChromaDB ë¶ˆëŸ¬ì˜¤ê¸° ---
vectorstore = Chroma(
    collection_name="campaign_media_chroma_hf",
    embedding_function=embedding_function,
    persist_directory="../chroma_db2"
)

print("ê¸°ì¡´ ChromaDB ë¡œë“œ ì™„ë£Œ!", file=sys.stderr)

async def db_query_tool_async(query: str, params: tuple = None):
    async with AsyncSessionLocal() as session:
        try:
            async with session.begin():
                stmt = text(query)
                result = await session.execute(stmt, params or ())
                return [dict(row._mapping) for row in result.fetchall()]
        except Exception as e:
            return []

def web_search_tool(query: str) -> str:
    return f"[WEB SEARCH RESULT for: {query}]"

def vectordb_search_tool(query: str, vectorstore, top_k: int = 3) -> str:
    results = vectorstore.similarity_search(query, k=top_k)
    combined_results = []
    for doc in results:
        content = doc.page_content
        content_lines = [f"- {line.strip()}" for line in content.split(",")]
        content_formatted = "\n".join(content_lines)
        image_url = doc.metadata.get("execution_image_url", "")
        if image_url.startswith("/images/"):
            image_url = "../" + image_url.lstrip("/")
        elif image_url == "":
            image_url = "[ì´ë¯¸ì§€ ì—†ìŒ]"
        content_with_image = f"{content_formatted}\n[ì´ë¯¸ì§€ ë³´ê¸°]({image_url})"
        combined_results.append(content_with_image)
    return "\n\n---\n\n".join(combined_results)

async def query_brand_and_sales_logs(brand_name: str) -> tuple[dict, Optional[dict]]:
    brand_info_list = await db_query_tool_async("SELECT * FROM brand WHERE brand_name = %s", (brand_name,))
    if not brand_info_list:
        return {}, None

    brand_info = brand_info_list[0]
    brand_id = brand_info["brand_id"]
    sales_logs = await db_query_tool_async(
        "SELECT * FROM sales_log WHERE brand_id = %s ORDER BY contact_time DESC LIMIT 1", (brand_id,)
    )
    return brand_info, sales_logs[0] if sales_logs else None

async def analyze_brand_and_needs(state: ProposalState):
    brand_name = state["brand_name"]
    brand_info, latest_sales_log = await query_brand_and_sales_logs(brand_name)
    if not brand_info:
        raise ValueError(f"ë¸Œëœë“œ '{brand_name}' ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    client_needs = latest_sales_log["client_needs_summary"] if latest_sales_log else "ìµœê·¼ ê³ ê° ìš”êµ¬ì‚¬í•­ ì •ë³´ ì—†ìŒ"
    recent_issues = brand_info.get("recent_brand_issues") or "ë¸Œëœë“œ ì´ìŠˆ ì •ë³´ ì—†ìŒ"
    sales_status = brand_info.get("sales_status") or "ìƒíƒœ ì •ë³´ ì—†ìŒ"

    return {
        **state,
        "brand_info": brand_info,
        "client_needs": client_needs,
        "recent_issues": recent_issues,
        "sales_status": sales_status
    }

def retrieve_previous_campaigns(state: ProposalState):
    client_needs = state.get("client_needs") or "ì˜¥ì™¸ ê´‘ê³  ì§‘í–‰ ì‚¬ë¡€"
    similar_cases = vectordb_search_tool(client_needs, vectorstore)
    return {**state, "previous_campaigns": similar_cases}

async def recommend_media(state: ProposalState):
    client_needs = state.get("client_needs") or ""
    db_results = await db_query_tool_async("SELECT * FROM media WHERE quantity > 0;")
    if not db_results:
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ì²´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    media_info = db_results

    media_json = json.dumps(db_results, ensure_ascii=False, default=lambda o: float(o) if isinstance(o, Decimal) else str(o))
    prompt = f"""
        ë‹¹ì‹ ì€ ì˜¥ì™¸ ê´‘ê³  ì „ë¬¸ ëŒ€í–‰ì‚¬ì˜ ì „ëµ ê¸°íšìì…ë‹ˆë‹¤.
        ë‹¤ìŒ ë¸Œëœë“œì˜ ê³ ê° ìš”êµ¬ì‚¬í•­ê³¼ ìœ ì‚¬ ì§‘í–‰ ì‚¬ë¡€ë¥¼ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì˜¥ì™¸ ê´‘ê³  ë§¤ì²´ 3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤.

        - ë¸Œëœë“œ ê³ ê° ìš”êµ¬ì‚¬í•­: {state.get('client_needs')}
        - ìœ ì‚¬ ì§‘í–‰ ì‚¬ë¡€ ìš”ì•½: {state.get('previous_campaigns')}

        ë‹¤ìŒì€ ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤:
        {media_json}
    """
    recommendation = await llm.invoke(prompt)
    recommendation_text = recommendation.content

    return {**state, "recommended_media": recommendation_text, "media_info": media_info}

def generate_proposal(state: ProposalState):
    import datetime
    import re
    doc = Document()
    doc.add_heading(f"{state['brand_name']} ì˜¥ì™¸ ê´‘ê³  ì œì•ˆì„œ", level=1)

    doc.add_heading("1. ê³ ê°ì‚¬ ì •ë³´", level=2)
    brand_info = state["brand_info"]
    if isinstance(brand_info, dict):
        for key, value in brand_info.items():
            doc.add_paragraph(f"- {key}: {value}")

    doc.add_heading("2. ìº í˜ì¸ ëª©í‘œ", level=2)
    client_needs = state["client_needs"]
    for item in re.split(r",|Â·|â€¢", client_needs):
        if item.strip():
            doc.add_paragraph(f"- {item.strip()}")

    doc.add_heading("3. ìœ ì‚¬ ì§‘í–‰ ì‚¬ë¡€", level=2)
    previous_campaigns = state["previous_campaigns"]
    cases = previous_campaigns.split("\n\n---\n\n")
    for idx, case in enumerate(cases, 1):
        doc.add_paragraph(f"- ì‚¬ë¡€ {idx}: {case}")

    doc.add_heading("4. ì¶”ì²œë§¤ì²´ ë° ì§‘í–‰ê³„íš", level=2)
    doc.add_paragraph(state["recommended_media"])

    doc.add_heading("5. ê²°ë¡ ", level=2)
    prompt = ChatPromptTemplate.from_template("""
    ë¸Œëœë“œëª…: {brand_name}
    ìº í˜ì¸ ëª©í‘œ: {client_needs}
    ì¶”ì²œ ë§¤ì²´: {recommended_media}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œì•ˆì„œì˜ ë§ˆë¬´ë¦¬ ê²°ë¡  ë¶€ë¶„ì„ ì‘ì„±í•˜ì„¸ìš”.
    """)
    chain = prompt | llm
    conclusion = chain.invoke(state).content
    doc.add_paragraph(conclusion)

    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"{state['brand_name']}_ì œì•ˆì„œ_{now}.docx"
    doc.save(file_name)

    return {**state, "proposal_text": conclusion, "proposal_file_path": file_name}

# --- ğŸ”— ê·¸ë˜í”„ êµ¬ì„± ---
graph = StateGraph(ProposalState)
graph.add_node("AnalyzeBrandAndNeeds", analyze_brand_and_needs)
graph.add_node("RecommendMedia", recommend_media)
graph.add_node("RetrievePreviousCampaigns", retrieve_previous_campaigns)
graph.add_node("GenerateProposal", generate_proposal)

graph.set_entry_point("AnalyzeBrandAndNeeds")
graph.add_edge("AnalyzeBrandAndNeeds", "RetrievePreviousCampaigns")
graph.add_edge("RetrievePreviousCampaigns", "RecommendMedia")
graph.add_edge("RecommendMedia", "GenerateProposal")
graph.set_finish_point("GenerateProposal")

proposal_graph = graph.compile()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--brand", required=True)
    args = parser.parse_args()

    try:
        initial_state = {"brand_name": args.brand}
        final_state = asyncio.run(proposal_graph.ainvoke(initial_state))
        
        print("âœ… ì œì•ˆì„œ ìƒì„± ì™„ë£Œ", file=sys.stderr)
        print(final_state["proposal_text"], file=sys.stderr)
        print(f"ğŸ“„ ì €ì¥ ê²½ë¡œ: {final_state['proposal_file_path']}", file=sys.stderr)

        result = {
            "success": True,
            "brand": args.brand,
            "file_path": final_state["proposal_file_path"],
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        print(json.dumps(result))
    except Exception as e:
        result = {"success": False, "error": str(e)}
        print(json.dumps(result))
